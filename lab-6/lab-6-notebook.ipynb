{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management, Analysis and Security</h1>\n",
    "\n",
    "<h2 align='center'> Lab 06 - Machine Learning - II</h2>\n",
    "\n",
    "*****\n",
    "\n",
    "In this lab, we will first guide you through some required interfaces of `sklearn`. We will implement and study **Decision Trees** and **Nearest-Neighbours Regression** implimented using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A **Decision Tree** is a **supervised** learning algorithm used for classification problems. In this algorithm, we split the original data into two or more homogeneous sets. This is done based on most significant attributes to make the resulting groups as distinct as possible. In decision analysis, a decision tree is used to visually and explicitly represent decisions and decision making. It uses a tree-like model of decisions. A decision tree is drawn with its **root** at the top and **branches** at the bottom. The branch end that doesn’t split anymore is the **decision / leaf**.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Pick an attribute to split at a non-terminal node (based on which attribute will provide the greatest **Information Gain**).\n",
    "2. Split examples into groups based on attribute value.\n",
    "3. For each group:\n",
    "    * **If** no examples – return majority from parent\n",
    "    * **Else If** all examples in same class – return class\n",
    "    * **Else** loop to Step 1\n",
    "\n",
    "\n",
    "### Properties\n",
    "\n",
    "* Internal nodes test attributes.\n",
    "* Branching is determined by attribute value.\n",
    "* Leaf nodes are outputs (class assignments).\n",
    "\n",
    "\n",
    "\n",
    "Let's try to make some predictions with such a decision tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sink or Float\n",
    "\n",
    "We will use the **Titanic Passenger Dataset** which has data related to 891 passengers and 11 features + the target variable (Survived) in the dataset. The dataset is provided under the `data` folder and has the following features:\n",
    "\n",
    "| **Feature**              |**Description**                                                                   |\n",
    "|--------------------------|----------------------------------------------------------------------------------|\n",
    "| PassengerId              | ID of Passenger in dataset                                                       |\n",
    "| Survived                 | Whether the passenger survived or not {0: No, 1: Yes}                            |\n",
    "| Pclass                   | Class of Travel (1,2 or 3)                                                       |\n",
    "| Name                     | Name of Passenger                                                                |\n",
    "| Sex                      | Gender of Passenger (Male or Female)                                             |\n",
    "| Age                      | Age of Passenger                                                                 |\n",
    "| Sibsp                    | Number of Siblings/Spouse aboard                                                 |\n",
    "| Parch                    | Number of Parent/Child aboard                                                    |\n",
    "| Ticket                   | Ticket Number                                                                    |\n",
    "| Fare                     | Cost of the Ticket                                                               |\n",
    "| Cabin                    | Cabin number of Passenger                                                        |\n",
    "| Embarked                 | Port which the Passenger embarked {C: Cherbourg, S: Southhampton, Q: Queenstown} |\n",
    "\n",
    "\n",
    "We are going to split this dataset into **train** and **test** data. We aim to build a **decision tree to predict whether a passenger will survive or not in the titanic crash**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the following steps to prepare the data for use:\n",
    "    - Load the dataset `titanic.csv` into a dataframe. Do a quick check on the type of data each column has. \n",
    "    - To make life easier for the decision tree, alter the **Sex** column such that `male` is replaced with `0` & `female` is replaced with `1`\n",
    "    - Clean up the **Age** column to remove any `NaN` values (**HINT**: Replace them with the mean of the entire column).\n",
    "    - For this exercise we would need only need the feature columns `Pclass`, `Sex`, `Age`, `SibSp` and `Parch`, and the result `Survived`. Split the DataFrame into two new variables, one being a DataFrame with the feature columns and the other holding the result data\n",
    "    - Using `sklearn`'s train-test-split function, split the data into a training set and testing set to evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name  Sex        Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    0  22.000000      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.000000      1   \n",
      "2                               Heikkinen, Miss. Laina    1  26.000000      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.000000      1   \n",
      "4                             Allen, Mr. William Henry    0  35.000000      0   \n",
      "5                                     Moran, Mr. James    0  29.699118      0   \n",
      "6                              McCarthy, Mr. Timothy J    0  54.000000      0   \n",
      "7                       Palsson, Master. Gosta Leonard    0   2.000000      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    1  27.000000      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)    1  14.000000      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut    1   4.000000      1   \n",
      "11                            Bonnell, Miss. Elizabeth    1  58.000000      0   \n",
      "12                      Saundercock, Mr. William Henry    0  20.000000      0   \n",
      "13                         Andersson, Mr. Anders Johan    0  39.000000      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina    1  14.000000      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)     1  55.000000      0   \n",
      "16                                Rice, Master. Eugene    0   2.000000      4   \n",
      "17                        Williams, Mr. Charles Eugene    0  29.699118      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...    1  31.000000      1   \n",
      "19                             Masselmani, Mrs. Fatima    1  29.699118      0   \n",
      "20                                Fynney, Mr. Joseph J    0  35.000000      0   \n",
      "21                               Beesley, Mr. Lawrence    0  34.000000      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"    1  15.000000      0   \n",
      "23                        Sloper, Mr. William Thompson    0  28.000000      0   \n",
      "24                       Palsson, Miss. Torborg Danira    1   8.000000      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...    1  38.000000      1   \n",
      "26                             Emir, Mr. Farred Chehab    0  29.699118      0   \n",
      "27                      Fortune, Mr. Charles Alexander    0  19.000000      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"    1  29.699118      0   \n",
      "29                                 Todoroff, Mr. Lalio    0  29.699118      0   \n",
      "..                                                 ...  ...        ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward    0  21.000000      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...    1  48.000000      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"    1  29.699118      8   \n",
      "864                             Gill, Mr. John William    0  24.000000      0   \n",
      "865                           Bystrom, Mrs. (Karolina)    1  42.000000      0   \n",
      "866                       Duran y More, Miss. Asuncion    1  27.000000      1   \n",
      "867               Roebling, Mr. Washington Augustus II    0  31.000000      0   \n",
      "868                        van Melkebeke, Mr. Philemon    0  29.699118      0   \n",
      "869                    Johnson, Master. Harold Theodor    0   4.000000      1   \n",
      "870                                  Balkic, Mr. Cerin    0  26.000000      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)    1  47.000000      1   \n",
      "872                           Carlsson, Mr. Frans Olof    0  33.000000      0   \n",
      "873                        Vander Cruyssen, Mr. Victor    0  47.000000      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)    1  28.000000      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"    1  15.000000      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian    0  20.000000      0   \n",
      "877                               Petroff, Mr. Nedelio    0  19.000000      0   \n",
      "878                                 Laleff, Mr. Kristo    0  29.699118      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)    1  56.000000      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)    1  25.000000      0   \n",
      "881                                 Markun, Mr. Johann    0  33.000000      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika    1  22.000000      0   \n",
      "883                      Banfield, Mr. Frederick James    0  28.000000      0   \n",
      "884                             Sutehall, Mr. Henry Jr    0  25.000000      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)    1  39.000000      0   \n",
      "886                              Montvila, Rev. Juozas    0  27.000000      0   \n",
      "887                       Graham, Miss. Margaret Edith    1  19.000000      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"    1  29.699118      1   \n",
      "889                              Behr, Mr. Karl Howell    0  26.000000      0   \n",
      "890                                Dooley, Mr. Patrick    0  32.000000      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin Embarked  \n",
      "0        0         A/5 21171    7.2500      29.6991        S  \n",
      "1        0          PC 17599   71.2833          C85        C  \n",
      "2        0  STON/O2. 3101282    7.9250      29.6991        S  \n",
      "3        0            113803   53.1000         C123        S  \n",
      "4        0            373450    8.0500      29.6991        S  \n",
      "5        0            330877    8.4583      29.6991        Q  \n",
      "6        0             17463   51.8625          E46        S  \n",
      "7        1            349909   21.0750      29.6991        S  \n",
      "8        2            347742   11.1333      29.6991        S  \n",
      "9        0            237736   30.0708      29.6991        C  \n",
      "10       1           PP 9549   16.7000           G6        S  \n",
      "11       0            113783   26.5500         C103        S  \n",
      "12       0         A/5. 2151    8.0500      29.6991        S  \n",
      "13       5            347082   31.2750      29.6991        S  \n",
      "14       0            350406    7.8542      29.6991        S  \n",
      "15       0            248706   16.0000      29.6991        S  \n",
      "16       1            382652   29.1250      29.6991        Q  \n",
      "17       0            244373   13.0000      29.6991        S  \n",
      "18       0            345763   18.0000      29.6991        S  \n",
      "19       0              2649    7.2250      29.6991        C  \n",
      "20       0            239865   26.0000      29.6991        S  \n",
      "21       0            248698   13.0000          D56        S  \n",
      "22       0            330923    8.0292      29.6991        Q  \n",
      "23       0            113788   35.5000           A6        S  \n",
      "24       1            349909   21.0750      29.6991        S  \n",
      "25       5            347077   31.3875      29.6991        S  \n",
      "26       0              2631    7.2250      29.6991        C  \n",
      "27       2             19950  263.0000  C23 C25 C27        S  \n",
      "28       0            330959    7.8792      29.6991        Q  \n",
      "29       0            349216    7.8958      29.6991        S  \n",
      "..     ...               ...       ...          ...      ...  \n",
      "861      0             28134   11.5000      29.6991        S  \n",
      "862      0             17466   25.9292          D17        S  \n",
      "863      2          CA. 2343   69.5500      29.6991        S  \n",
      "864      0            233866   13.0000      29.6991        S  \n",
      "865      0            236852   13.0000      29.6991        S  \n",
      "866      0     SC/PARIS 2149   13.8583      29.6991        C  \n",
      "867      0          PC 17590   50.4958          A24        S  \n",
      "868      0            345777    9.5000      29.6991        S  \n",
      "869      1            347742   11.1333      29.6991        S  \n",
      "870      0            349248    7.8958      29.6991        S  \n",
      "871      1             11751   52.5542          D35        S  \n",
      "872      0               695    5.0000  B51 B53 B55        S  \n",
      "873      0            345765    9.0000      29.6991        S  \n",
      "874      0         P/PP 3381   24.0000      29.6991        C  \n",
      "875      0              2667    7.2250      29.6991        C  \n",
      "876      0              7534    9.8458      29.6991        S  \n",
      "877      0            349212    7.8958      29.6991        S  \n",
      "878      0            349217    7.8958      29.6991        S  \n",
      "879      1             11767   83.1583          C50        C  \n",
      "880      1            230433   26.0000      29.6991        S  \n",
      "881      0            349257    7.8958      29.6991        S  \n",
      "882      0              7552   10.5167      29.6991        S  \n",
      "883      0  C.A./SOTON 34068   10.5000      29.6991        S  \n",
      "884      0   SOTON/OQ 392076    7.0500      29.6991        S  \n",
      "885      5            382652   29.1250      29.6991        Q  \n",
      "886      0            211536   13.0000      29.6991        S  \n",
      "887      0            112053   30.0000          B42        S  \n",
      "888      2        W./C. 6607   23.4500      29.6991        S  \n",
      "889      0            111369   30.0000         C148        C  \n",
      "890      0            370376    7.7500      29.6991        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "df1 = pd.read_csv('data/titanic.csv')\n",
    "df1['Sex'] = df1['Sex'].map({'female': 1, 'male': 0})\n",
    "age_mean = df1['Age'].mean()\n",
    "df1 = df1.fillna(age_mean)\n",
    "\n",
    "\n",
    "x = df1[['Pclass','Sex','Age','SibSp']]\n",
    "y = df1[['Survived']]\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(x, y,\n",
    "        test_size=0.2, random_state=None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with a decision tree object?\n",
    "\n",
    "| Object Method | Description |\n",
    "| --- |:---:|\n",
    "| `dt.apply()` | **Returns the index of the leaf that each sample is predicted as.** |\n",
    "| `dt.decision_path()` | **Return the decision path in the tree** |\n",
    "| `dt.fit()` | **Build a decision tree classifier from the training set.** |\n",
    "| `dt.predict()` | **Predict class value for X.** |\n",
    "| `dt.score()` | **Returns the mean accuracy on the given test data and labels.** |\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Using the `sklearn` module, make a decision tree object and fit it to the `training` dataset. Determine the accuracy of this model on the `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130 179  69  65 289  23 130  63 136  92 237  93 207  98 130  92 193 122\n",
      " 229 106 130 220 130 174 188 177 261 130 261 216 169 188 211  65  85 188\n",
      " 179 121  83 178 163 160 193 220 165 190 112  93 111 188   5  96 111 264\n",
      " 190 102 174 148 130  65 174 261 145 157 278 266 207 234 178  56 131 190\n",
      " 130  48 102 160 190 198 289 174  65  65 160  98 190 131  98 190  98 133\n",
      " 250  33 237 193  25 130 278  65 278 190 190 291 160  39 130  33 122 190\n",
      " 145 179 130  48 261  34 174 223 125 117  33 141 130  98  36  87  24 253\n",
      "  93  36 190  85 111  34 190 190 188  65 111  36 142 190  92 133 261 130\n",
      " 160 278 188 193 167 190 125 178 160  36  33  39  98  92 228  98 130  98\n",
      " 178   8 106 169 130 200 158 130 190 234 261 125 261  98 106  33 250]\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "# Labels | Y values\n",
    "dty_labels = df1['Survived']\n",
    "\n",
    "# Features | X values\n",
    "dt_features = df1[['Pclass','Sex','Age','SibSp']]\n",
    "\n",
    "# Classification\n",
    "dt_classifier = DecisionTreeClassifier(criterion=\"entropy\", presort=True)\n",
    "\n",
    "dtx_train, dtx_test, dty_train, dty_test = train_test_split(dt_features, dty_labels, test_size=0.2)\n",
    "dt_classifier.fit(dtx_train, dty_train)\n",
    "applied = dt_classifier.apply(dtx_test)\n",
    "print(applied)\n",
    "dotf = export_graphviz(dt_classifier, out_file='out.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How would the accuracy be affected if you increase or decrease the `DecisionTreeClassifier` object parameters like `max_depth` and `min_samples_leaf`? You are welcome to try making the `DecisionTreeClassifier` object with different values of `max_depth` and `min_samples_leaf` and compare your model's accuracy. But, make sure you can justify the change in accuracy for the increase or decrease in these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 92  73  85  63  63  47 105  81 106  47  52  47  55   7 104  47  85  85\n",
      "  47  47  22  47 106  20  25  31  82  81  82  57  85  83   8  19  53  85\n",
      "  47  47 107  63  91  85  72  20  39  47  47  71  85  54 104  52  47  93\n",
      "  47  39  91  52  47  47  54  20  68  47  68  71  47  71 116 107  93  39\n",
      "  53 106  82  47  52  52  55  52  85 115  20  25  47  87  57  96  39  81\n",
      "  52  47  29  47 119   8 119  31  49 106 122   8 109  39 106  47  85  71\n",
      "  62  62  88 121  20  48 106  47  47  68  20  81  20 119  20  71  47  47\n",
      " 109  29  39  91  68  68  47  82  72  39   4  47  47 107  81  18  47  47\n",
      "  20  68  57 115  81  83  81  52  16  81  85  79  41 107  48  47  81   4\n",
      "  25 116 107  71  52  81  85  39  52 104  81  24  71  52  63  85   5]\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "# Classification\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, criterion=\"entropy\", presort=True)\n",
    "\n",
    "dtx_train, dtx_test, dty_train, dty_test = train_test_split(dt_features, dty_labels, test_size=0.2)\n",
    "dt_classifier.fit(dtx_train, dty_train)\n",
    "applied = dt_classifier.apply(dtx_test)\n",
    "print(applied)\n",
    "dotf = export_graphviz(dt_classifier, out_file='out.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justification\n",
    "- As necessary. Use this to put down notes that you can come back to quickly later when you're studying !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scikit-Learn provides you with an easy way to visualize and export this **Decision Tree**. Export this decision tree to your machine and visually inspect it (**HINT**: This was performed in the lectures)\n",
    "\n",
    "Note: If you're doing this at home, you may require extra files to compile the dot file to visually inspect. The steps are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "#copy code of out.dot, go to graphviz online and paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the `.dot` file (using Graphviz)\n",
    "\n",
    "##### OSX (Using Homebrew)\n",
    "- In an open terminal window:\n",
    "    - `brew install graphviz`\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)\n",
    "\n",
    "##### Ubuntu \n",
    "- In an open terminal window\n",
    "    - `sudo apt install graphviz` (if not already installed, can sometimes come standard) **(On CECS computers, it will be automatically installed)**\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)\n",
    "    \n",
    "##### Windows\n",
    "- Install the appropiate packages from (here)[https://graphviz.gitlab.io/_pages/Download/Download_windows.html]\n",
    "- Ensure Graphiz is within your PATH\n",
    "- Using Powershell\n",
    "    - `dot -Tpng decision_tree.dot -o decision_tree.png` (when in the directory of the `.dot` file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## k-Nearest Neighbours\n",
    "\n",
    "A **k-Nearest Neighbours** is a **supervised learning** algorithm. It is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. \n",
    "\n",
    "KNN is a **non-parametric learning algorithm**, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc.\n",
    "\n",
    "<img src='./images/knn_classification.jpg'>\n",
    "\n",
    "Source: [E_blog - K-Nearest Neighbour(KNN) Classification](https://zeidigital.wordpress.com/2016/08/13/k-nearest-neighbour-classification-algorithm-implementation-in-python/)\n",
    "\n",
    "Given a dataset with **x and y**, Nearest Neighbours Regression can be used to:\n",
    "\n",
    "* Build a predictive model to predict future values of **x<sub>i</sub>** without a **y<sub>i</sub>** value.\n",
    "* Build a model without assuming any parameters, **K** is hyperparameter.\n",
    "* It can be seen as a baseline method to compare your complex models.\n",
    "\n",
    "Lets try doing some predictions using a k-Nearest Neighbours algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Want to buy\n",
    "\n",
    "We have provided a dataset of 200 consumers and 3 features and a target variable in the dataset. The dataset is provided under the `data` folder and has the following features:\n",
    "\n",
    "| **Feature**              |**Description**                                                                   |\n",
    "|--------------------------|----------------------------------------------------------------------------------|\n",
    "| CustomerID               | Unique ID assigned to the customer                                               |\n",
    "| Gender                   | Gender of the Shopper (Male or Female)                                           |\n",
    "| Age                      | Age of the Shopper                                                               |\n",
    "| Annual Income            | Annual Income of the customer (in thousands)                                     |\n",
    "| Will Buy                 | Whether the customer will buy item x {Yes: 1, No: 0}                             |\n",
    "\n",
    "#### Scenario\n",
    "You run a local grocery store and have been approached regarding stocking a products. Product-x is being marketed as the biggest thing since sliced bread in the next town over, although you're unsure if your customers will be as responsive. You decide to purchase a test batch and allow your 200 loyalty members to review the product, and use their buying statistics to determine whether you should stock the product permanently. Loyalty memebers will inform you whether they brought the product or not, and you can use this to predict whether other customers will do the same.\n",
    "\n",
    "Note: This is another example of a binary classification problem, as you are trying to determine whether the answer will be \"yes\" or \"no\" given a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the following steps to prepare the data for use:\n",
    "    - Load the dataset `buying_cx.csv` into a dataframe, using CustomerID as the index column\n",
    "    - As with the previous task, alter the `Gender` column to reflect 0 meaning Male & 1 meaning Female (impliment this using a different method to the previous question) \n",
    "        - **Extension**: Use a LabelEncoder or the like from the `sklearn` module to change the values.\n",
    "    - Split the data into a training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "df2 = pd.read_csv('data/buying_cx.csv')\n",
    "\n",
    "df2['Gender'] = df2['Gender'].map({'Female': 1, 'Male': 0})\n",
    "\n",
    "# Features | X values\n",
    "X = df2[['Gender','Age','Annual Income (k$)']]\n",
    "\n",
    "# Labels | Y values\n",
    "y = df2[['Will Buy']]\n",
    "y = np.ravel(y)\n",
    "\n",
    "\n",
    "#length = X.shape[0]\n",
    "#print(length)\n",
    "#X_train = X[:int(length*0.8)]\n",
    "#y_train = y[:int(length*0.8)]\n",
    "#X_test = X[int(length*0.8):]\n",
    "#y_test = y[int(length*0.8):]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "        test_size=0.2, random_state=None, shuffle=True) #train80%, test 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with a k-Nearest Neighbors object?\n",
    "While there are other methods, the main functions you will require for this exercise are the following:\n",
    "\n",
    "| Method | Description |\n",
    "| --- |:---:|\n",
    "| `.fit()` | **Build a decision tree classifier from the training set.** |\n",
    "| `.predict()` | **Predict class value for X.** |\n",
    "| `.score()` | **Returns the mean accuracy on the given test data and labels.** |\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Using the k-Nearest Neighbors class from `sklearn`, impliment a k-Nearest Neighbors classifier where it checks the 5 closest neighbours, and determine the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1\n",
      " 0 0 0]\n",
      "[0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1\n",
      " 0 0 1]\n",
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predictions = knn.predict(X_test)\n",
    "\n",
    "# compare visually\n",
    "print(y_test)\n",
    "print(y_predictions)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. As k is a hyperparameter that we specify when we create the model, it is possible to adjust the number of neighbours the model will check for a solution. Create 3 more models, each with a different k value (k=1, k=50, k=150) and compare the scores of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "#k = 1\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predictions = knn.predict(X_test)\n",
    "\n",
    "# compare visually\n",
    "#print(y_test)\n",
    "#print(y_predictions)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "#k = 50\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predictions = knn.predict(X_test)\n",
    "\n",
    "# compare visually\n",
    "#print(y_test)\n",
    "#print(y_predictions)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "#k = 150\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=150)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_predictions = knn.predict(X_test)\n",
    "\n",
    "# compare visually\n",
    "#print(y_test)\n",
    "#print(y_predictions)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these scores to when we tried for the 5 closest neighbours, what does this tell us about the optimal number of neighbours? Will the optimal number of neighbours remain constant over different divisions of testing and training data? Discuss this with other members of the course or your tutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "- Insert Notes from your discussion here as necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
